# üîç Guia Completo: Servi√ßos e Imagens para Migra√ß√£o

**Data**: 2025-10-29
**Objetivo**: Identificar servi√ßos internos vs AWS, imagens para migra√ß√£o e configura√ß√£o GenAI

---

## üì¶ 1. IMAGENS DOCKER PARA MIGRA√á√ÉO

### 1.1 Imagens Identificadas

| Imagem Original | Componente | Tamanho Aprox | Prioridade |
|----------------|------------|---------------|------------|
| `milvusdb/milvus:v2.5.15` | Milvus (5 componentes) | ~1GB | üî¥ ALTA |
| `docker.io/milvusdb/etcd:3.5.18-r1` | Etcd | ~150MB | üî¥ ALTA |
| `docker.io/bitnami/kafka:3.1.0-debian-10-r52` | Kafka | ~500MB | üî¥ ALTA |
| `docker.io/bitnami/zookeeper:3.7.0-debian-10-r320` | Zookeeper | ~300MB | üî¥ ALTA |
| `icr.io/mjc-cr/mmjc-airflow-service:latest` | Airflow (todos) | ~2GB | üî¥ ALTA |
| `quay.io/prometheus/statsd-exporter:v0.28.0` | StatsD | ~20MB | üü° BAIXA (p√∫blica) |

**Total estimado**: ~6-8GB de imagens

### 1.2 Imagens que DEVEM ser Migradas

**Prioridade ALTA** (imagens de registries privados ou vers√µes espec√≠ficas):
```bash
# Milvus Stack (todas em docker.io - podem ou n√£o ser migradas)
milvusdb/milvus:v2.5.15
docker.io/milvusdb/etcd:3.5.18-r1
docker.io/bitnami/kafka:3.1.0-debian-10-r52
docker.io/bitnami/zookeeper:3.7.0-debian-10-r320

# Airflow (DEVE ser migrado - est√° em registry privado IBM)
icr.io/mjc-cr/mmjc-airflow-service:latest
```

**Prioridade BAIXA** (imagens p√∫blicas - podem permanecer):
```bash
quay.io/prometheus/statsd-exporter:v0.28.0
```

### 1.3 Op√ß√µes de Migra√ß√£o de Imagens

#### Op√ß√£o A: Migrar para AWS ECR (Recomendado para AWS)

```bash
# 1. Login no ECR
aws ecr get-login-password --region us-east-1 | \
    docker login --username AWS --password-stdin \
    ${AWS_ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com

# 2. Criar reposit√≥rios
aws ecr create-repository --repository-name milvus
aws ecr create-repository --repository-name etcd
aws ecr create-repository --repository-name kafka
aws ecr create-repository --repository-name zookeeper
aws ecr create-repository --repository-name mmjc-airflow-service

# 3. Migrar imagens (usar script automatizado)
./scripts/migrate-container-images.sh
```

#### Op√ß√£o B: Migrar para IBM ICR (conta Ita√∫)

```bash
# 1. Login na conta Ita√∫
~/ibm-login-itau

# 2. Criar namespace
ibmcloud cr namespace-add itau-airflow

# 3. Configurar registry
export TARGET_REGISTRY=icr.io/itau-airflow

# 4. Migrar
./scripts/migrate-container-images.sh
```

#### Op√ß√£o C: Usar Registries Originais (Tempor√°rio - N√ÉO RECOMENDADO)

```bash
# Apenas para testes r√°pidos
# Requer acesso ao icr.io/mjc-cr (IBM Cloud original)
```

---

## üèóÔ∏è 2. SERVI√áOS: INTERNO vs AWS

### 2.1 Servi√ßos INTERNOS ao Cluster (N√£o precisam AWS)

Estes rodam completamente dentro do Kubernetes:

| Servi√ßo | Namespace | Pods | Storage | AWS Dependency |
|---------|-----------|------|---------|----------------|
| **Milvus DataNode** | mmjc-test | 2 | PVC (EBS) | ‚ùå Nenhuma |
| **Milvus IndexNode** | mmjc-test | 1 | PVC (EBS) | ‚ùå Nenhuma |
| **Milvus QueryNode** | mmjc-test | 1 | PVC (EBS) | ‚ùå Nenhuma |
| **Milvus MixCoord** | mmjc-test | 1 | PVC (EBS) | ‚ùå Nenhuma |
| **Milvus Proxy** | mmjc-test | 1 | - | ‚ùå Nenhuma |
| **Etcd** | mmjc-test | 3 | PVC (EBS) | ‚ùå Nenhuma |
| **Kafka** | mmjc-test | 3 | PVC (EBS) | ‚ùå Nenhuma |
| **Zookeeper** | mmjc-test | 3 | PVC (EBS) | ‚ùå Nenhuma |
| **MinIO** | mmjc-test | 4 | PVC (EBS) | üü° Opcional S3 |

**Caracter√≠sticas**:
- ‚úÖ Rodam como StatefulSets ou Deployments
- ‚úÖ Usam PersistentVolumeClaims (EBS)
- ‚úÖ Comunica√ß√£o intra-cluster
- ‚úÖ N√£o dependem de servi√ßos AWS externos

### 2.2 Servi√ßos AWS EXTERNOS (Depend√™ncias Obrigat√≥rias)

#### üî¥ OBRIGAT√ìRIOS

| Servi√ßo AWS | Para que serve | Usado por | Alternativa Interna |
|-------------|----------------|-----------|---------------------|
| **RDS PostgreSQL 15** | Metadata DB | Airflow | ‚ùå N√£o (complexo) |
| **Redis as Cache (Redis 7)** | Message Broker | Airflow (Celery) | ‚ùå N√£o (recomendado externo) |
| **S3 Buckets** | Logs/DAGs | Airflow | üü° Pode usar PVC (n√£o recomendado) |
| **EBS Volumes** | Persistent Storage | Todos StatefulSets | ‚ùå N√£o (managed pelo EKS) |
| **ALB (Load Balancer)** | Ingress | Airflow/Milvus | üü° Pode usar NodePort (n√£o recomendado) |

#### üü° OPCIONAIS (Mas Recomendados)

| Servi√ßo AWS | Para que serve | Usado por | Alternativa |
|-------------|----------------|-----------|-------------|
| **S3** | Object Storage | Milvus (substitui MinIO) | ‚úÖ MinIO interno |
| **ECR** | Container Registry | Todas imagens | ‚úÖ IBM ICR ou Docker Hub |
| **ACM** | Certificados SSL | ALB/Ingress | ‚úÖ Cert-manager interno |
| **Route 53** | DNS | ALB | ‚úÖ DNS externo qualquer |
| **CloudWatch** | Logs/M√©tricas | Monitoramento | ‚úÖ Prometheus/Grafana |

### 2.3 Diagrama de Depend√™ncias

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        AWS EKS CLUSTER                        ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ         INTERNO (N√£o precisa AWS externo)               ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Milvus (5 componentes)                              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Etcd (3 pods)                                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Kafka (3 pods)                                      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Zookeeper (3 pods)                                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ MinIO (4 pods) ‚Üê Opcional substituir por S3        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Storage: PVCs ‚Üí EBS (AWS)                            ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ         AIRFLOW (Precisa AWS externo)                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ API Server    ‚îÄ‚îÄ‚Üí  RDS PostgreSQL (AWS)            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Scheduler     ‚îÄ‚îÄ‚Üí  Redis as Cache (AWS)           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Workers       ‚îÄ‚îÄ‚Üí  S3 Buckets (AWS)                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Triggerer                                           ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ      SERVI√áOS AWS EXTERNOS          ‚îÇ
        ‚îÇ                                     ‚îÇ
        ‚îÇ  ‚Ä¢ RDS PostgreSQL (OBRIGAT√ìRIO)    ‚îÇ
        ‚îÇ  ‚Ä¢ Redis as Cache (OBRIGAT√ìRIO)    ‚îÇ
        ‚îÇ  ‚Ä¢ S3 (OBRIGAT√ìRIO para Airflow)   ‚îÇ
        ‚îÇ  ‚Ä¢ EBS (AUTOM√ÅTICO pelo EKS)       ‚îÇ
        ‚îÇ  ‚Ä¢ ALB (OBRIGAT√ìRIO para ingress)  ‚îÇ
        ‚îÇ  ‚Ä¢ ECR (OPCIONAL para imagens)     ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ü§ñ 3. CONFIGURA√á√ÉO GenAI com BEDROCK SONNET

### 3.1 Arquitetura GenAI

Atualmente **N√ÉO encontramos** refer√™ncias expl√≠citas a:
- Bedrock
- Anthropic
- Claude
- Sonnet
- LLM
- GenAI

no c√≥digo-fonte atual.

### 3.2 Configura√ß√£o Recomendada (Para Implementar)

Se voc√™ quiser adicionar GenAI com **AWS Bedrock + Claude Sonnet**:

#### A. Configurar AWS Bedrock Access

```bash
# 1. Habilitar modelo no Bedrock
aws bedrock list-foundation-models --region us-east-1 | \
    jq '.modelSummaries[] | select(.modelId | contains("claude"))'

# 2. Solicitar acesso ao modelo (console AWS)
# AWS Console ‚Üí Bedrock ‚Üí Model access ‚Üí Request access
# Modelo: claude-3-5-sonnet-20241022 (ou mais recente)
```

#### B. Criar IAM Policy para Bedrock

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "bedrock:InvokeModel",
        "bedrock:InvokeModelWithResponseStream"
      ],
      "Resource": [
        "arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-5-sonnet-20241022-v2:0"
      ]
    }
  ]
}
```

#### C. Adicionar ao Terraform

```hcl
# terraform/modules/bedrock/main.tf
resource "aws_iam_policy" "bedrock_access" {
  name        = "${var.project_name}-bedrock-access"
  description = "Allow Airflow to invoke Bedrock models"

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "bedrock:InvokeModel",
          "bedrock:InvokeModelWithResponseStream"
        ]
        Resource = [
          "arn:aws:bedrock:${var.aws_region}::foundation-model/anthropic.claude-*"
        ]
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "bedrock_to_airflow" {
  role       = var.airflow_role_name
  policy_arn = aws_iam_policy.bedrock_access.arn
}
```

#### D. Configurar no Airflow (DAG Example)

```python
# dags/genai_example.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import boto3
import json

def invoke_claude_sonnet(**context):
    """Invoca Claude Sonnet via Bedrock"""

    bedrock = boto3.client(
        service_name='bedrock-runtime',
        region_name='us-east-1'
    )

    prompt = "Analise este texto: ..."

    body = json.dumps({
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": 4096,
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ]
    })

    response = bedrock.invoke_model(
        modelId="anthropic.claude-3-5-sonnet-20241022-v2:0",
        body=body
    )

    response_body = json.loads(response['body'].read())
    return response_body['content'][0]['text']

with DAG(
    'genai_bedrock_example',
    start_date=datetime(2025, 1, 1),
    schedule_interval='@daily',
    catchup=False
) as dag:

    task_genai = PythonOperator(
        task_id='invoke_claude',
        python_callable=invoke_claude_sonnet
    )
```

#### E. Adicionar boto3 ao Container Airflow

```dockerfile
# Se precisar rebuildar a imagem
FROM apache/airflow:2.x.x

RUN pip install --no-cache-dir \
    boto3>=1.28.0 \
    botocore>=1.31.0
```

Ou via requirements.txt:
```txt
boto3>=1.28.0
botocore>=1.31.0
```

#### F. Vari√°veis de Ambiente

```yaml
# kustomize/airflow-test/base/deployment.yaml
env:
  - name: AWS_DEFAULT_REGION
    value: "us-east-1"
  - name: BEDROCK_MODEL_ID
    value: "anthropic.claude-3-5-sonnet-20241022-v2:0"
```

### 3.3 Modelos Dispon√≠veis (Claude/Bedrock)

| Modelo | Model ID | Tokens | Custo Aprox | Uso |
|--------|----------|--------|-------------|-----|
| **Claude 3.5 Sonnet** | `anthropic.claude-3-5-sonnet-20241022-v2:0` | 200K | $$$ | Produ√ß√£o |
| Claude 3 Opus | `anthropic.claude-3-opus-20240229-v1:0` | 200K | $$$$ | Tarefas complexas |
| Claude 3 Sonnet | `anthropic.claude-3-sonnet-20240229-v1:0` | 200K | $$ | Balanceado |
| Claude 3 Haiku | `anthropic.claude-3-haiku-20240307-v1:0` | 200K | $ | R√°pido/barato |

**Recomenda√ß√£o**: Claude 3.5 Sonnet (v2) para melhor custo-benef√≠cio

### 3.4 Seguran√ßa GenAI

```bash
# 1. NUNCA expor API keys no c√≥digo
# 2. Usar IRSA (IAM Roles for Service Accounts)
# 3. Limitar modelos espec√≠ficos na policy
# 4. Monitorar custos no AWS Cost Explorer
# 5. Implementar rate limiting
```

---

## üìù 4. INSTRU√á√ïES DE MIGRA√á√ÉO

### 4.1 Passo a Passo Completo

#### ETAPA 1: Migrar Imagens (OBRIGAT√ìRIO)

```bash
# 1. Configurar target registry
vim config/migration.env
# Adicionar:
# TARGET_REGISTRY=123456789012.dkr.ecr.us-east-1.amazonaws.com

# 2. Executar migra√ß√£o de imagens
./scripts/migrate-container-images.sh

# 3. Verificar
docker images | grep ecr
```

#### ETAPA 2: Atualizar Refer√™ncias

```bash
# Editar kustomization.yaml
vim kustomize/airflow-test/kustomization.yaml
vim kustomize/milvus/kustomization.yaml

# Adicionar se√ß√£o images:
images:
  - name: icr.io/mjc-cr/mmjc-airflow-service
    newName: ${AWS_ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/mmjc-airflow-service
    newTag: latest
  - name: milvusdb/milvus
    newName: ${AWS_ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/milvus
    newTag: v2.5.15
```

#### ETAPA 3: Deploy Servi√ßos AWS

```bash
# 1. Criar recursos AWS
cd terraform/environments/dev
terraform init
terraform apply

# Outputs esperados:
# - rds_endpoint
# - redis_endpoint
# - s3_bucket_names
```

#### ETAPA 4: Deploy Kubernetes

```bash
# 1. Deploy Milvus (interno - n√£o precisa AWS al√©m de EBS)
kubectl apply -k kustomize/milvus/

# 2. Verificar
kubectl get pods -n mmjc-test

# 3. Deploy Airflow (precisa RDS + Redis + S3)
kubectl apply -k kustomize/airflow-test/

# 4. Verificar
kubectl get pods -n airflow-test
```

#### ETAPA 5: Configurar GenAI (OPCIONAL)

```bash
# 1. Habilitar Bedrock
aws bedrock list-foundation-models --region us-east-1

# 2. Adicionar m√≥dulo Terraform
# (ver se√ß√£o 3.2.C acima)

# 3. Deploy atualizado
terraform apply
kubectl rollout restart deployment -n airflow-test
```

### 4.2 Checklist de Valida√ß√£o

```bash
# ‚úÖ Imagens migradas
docker pull ${AWS_ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/milvus:v2.5.15

# ‚úÖ RDS acess√≠vel
aws rds describe-db-instances --db-instance-identifier itau-airflow-postgres

# ‚úÖ Redis acess√≠vel
aws elasticache describe-cache-clusters --cache-cluster-id itau-airflow-redis  # Redis as Cache

# ‚úÖ S3 buckets criados
aws s3 ls | grep airflow

# ‚úÖ Pods running
kubectl get pods -n mmjc-test
kubectl get pods -n airflow-test

# ‚úÖ Services expostos
kubectl get ingress -A

# ‚úÖ Storage bound
kubectl get pvc -A
```

---

## üéØ 5. RESUMO EXECUTIVO

### O que √© INTERNO (n√£o precisa AWS externo):

- ‚úÖ Milvus (todos 5 componentes)
- ‚úÖ Etcd (3 pods)
- ‚úÖ Kafka (3 pods)
- ‚úÖ Zookeeper (3 pods)
- ‚úÖ MinIO (4 pods) - mas pode ser substitu√≠do por S3

**Storage**: Usam EBS via PVCs (managed pelo EKS)

### O que PRECISA AWS externo:

- üî¥ RDS PostgreSQL (Airflow metadata)
- üî¥ Redis as Cache (Airflow Celery)
- üî¥ S3 (Airflow logs/DAGs)
- üî¥ ALB (Ingress controller)
- üî¥ EBS (Persistent volumes)
- üü° ECR (opcional - pode usar IBM ICR)

### Imagens para migrar:

```
OBRIGAT√ìRIAS:
- icr.io/mjc-cr/mmjc-airflow-service:latest ‚Üí ECR ou IBM ICR

RECOMENDADAS:
- milvusdb/milvus:v2.5.15
- docker.io/milvusdb/etcd:3.5.18-r1
- docker.io/bitnami/kafka:3.1.0-debian-10-r52
- docker.io/bitnami/zookeeper:3.7.0-debian-10-r320

OPCIONAL:
- quay.io/prometheus/statsd-exporter:v0.28.0 (p√∫blica)
```

### GenAI/Bedrock:

- ‚ùå N√£o configurado atualmente
- ‚úÖ Pode ser adicionado seguindo se√ß√£o 3
- üéØ Modelo recomendado: `anthropic.claude-3-5-sonnet-20241022-v2:0`

---

## üìö Refer√™ncias

- [AWS Bedrock Documentation](https://docs.aws.amazon.com/bedrock/)
- [Claude on Bedrock](https://docs.anthropic.com/en/api/claude-on-amazon-bedrock)
- [Milvus Documentation](https://milvus.io/docs)
- [Apache Airflow on EKS](https://airflow.apache.org/docs/)

---

**√öltima atualiza√ß√£o**: 2025-10-29
**Pr√≥ximos passos**: Execute `./migrate.sh` para iniciar migra√ß√£o autom√°tica
